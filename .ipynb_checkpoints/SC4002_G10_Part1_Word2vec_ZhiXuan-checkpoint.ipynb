{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b24801-1589-4c2e-8061-65314ff5edb9",
   "metadata": {
    "id": "29b24801-1589-4c2e-8061-65314ff5edb9"
   },
   "source": [
    "# Part 0: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383635c4-8206-46fd-b38f-fb063aac049e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "383635c4-8206-46fd-b38f-fb063aac049e",
    "outputId": "dff6d13f-2d46-4c4b-99be-c63d57a74cbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\zhixu\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f9792a-40de-4947-b9dd-7f9818bab9bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1f9792a-40de-4947-b9dd-7f9818bab9bf",
    "outputId": "390ab8ea-4161-47ec-a44e-ef2675bb858a"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f0ad4a-2abd-4cab-95ef-60ac18ee88c4",
   "metadata": {
    "id": "a7f0ad4a-2abd-4cab-95ef-60ac18ee88c4"
   },
   "source": [
    "# Part 1: Preparing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8acb20b4-0636-4a8a-9e32-346e4e96d84c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8acb20b4-0636-4a8a-9e32-346e4e96d84c",
    "outputId": "3ff3be43-803a-44f1-c96a-06248f1541f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhixu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d45b6-02c5-415d-bcd7-a8e7d1e70b52",
   "metadata": {
    "id": "a42d45b6-02c5-415d-bcd7-a8e7d1e70b52"
   },
   "source": [
    "## Question 1. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7dbca-b557-4bc8-8a77-1cd485545bf4",
   "metadata": {
    "id": "81b7dbca-b557-4bc8-8a77-1cd485545bf4"
   },
   "source": [
    "### (a) What is the size of the vocabulary formed from your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f167d3b6-78ae-49bc-9aa1-2e71861188ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f167d3b6-78ae-49bc-9aa1-2e71861188ee",
    "outputId": "4ec5a748-6841-4f69-8ad0-8203090f1940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 183968\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "tokenized_sentences = [nltk.tokenize.word_tokenize(sentence['text'].lower()) for sentence in train_dataset]\n",
    "\n",
    "for sentence in train_dataset:\n",
    "    tokens.extend(nltk.tokenize.word_tokenize(sentence['text'].lower()))\n",
    "\n",
    "print ('Number of tokens: '+ str(len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8241a09-af7d-474f-a17f-cdc200fcecaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8241a09-af7d-474f-a17f-cdc200fcecaf",
    "outputId": "599ebde5-168c-40e9-f3b9-1350b2c2d7be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token types (Vocabulary Size): 18029\n"
     ]
    }
   ],
   "source": [
    "# List of all distinct words found in the tokens list\n",
    "vocab = list(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "print ('Number of token types (Vocabulary Size): '+ str(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56465fdc-aeb5-43ac-ae9c-ee6cec0a0512",
   "metadata": {
    "id": "56465fdc-aeb5-43ac-ae9c-ee6cec0a0512"
   },
   "source": [
    "The size of vocabulary formed from the training data is 18029."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73eb4d-0962-4116-8c60-931f16e8d5dd",
   "metadata": {},
   "source": [
    "### (b) We use OOV (out-of-vocabulary) to refer to those words appeared in the training data but  not in the Word2vec (or Glove) dictionary. How many OOV words exist in your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "frdSab0eQaP5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frdSab0eQaP5",
    "outputId": "4a8f1dde-9e5f-4879-acc4-105dd0041f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8841\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Set parameters for the Word2Vec model\n",
    "embedding_dim = 100  # Adjust this dimension as needed\n",
    "window_size = 5  # Context window size\n",
    "min_count = 2  # Ignores words with total frequency lower than this\n",
    "\n",
    "# Train the Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=embedding_dim, window=window_size, min_count=min_count, workers=4)\n",
    "\n",
    "# Vocabulary size after training (vocabulary size of word2vec)\n",
    "vocab_size = len(word2vec_model.wv)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ig2lYogER1Rh",
   "metadata": {
    "id": "Ig2lYogER1Rh"
   },
   "source": [
    "Number of OOV words = 18029 - 8841 = 9188."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "_xJW9NxGOaWt",
   "metadata": {
    "id": "_xJW9NxGOaWt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (8841, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize the embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Map words in the Word2Vec vocabulary to the embedding matrix\n",
    "word_to_idx = {word: idx for idx, word in enumerate(word2vec_model.wv.index_to_key)}\n",
    "for word, idx in word_to_idx.items():\n",
    "    embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc6d20-59b2-46cd-a934-9e3268a5303b",
   "metadata": {
    "id": "7fJbvY-vR1DB"
   },
   "source": [
    "### (c) The existence of the OOV words is one of the well-known limitations of Word2vec (or Glove). Without using any transformer-based language models (e.g., BERT, GPT, T5), what do you think is the best strategy to mitigate such limitation? Implement your solution in your source code. Show the corresponding code snippet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1fc8d-12cb-4fb4-83ed-62a232aebd9a",
   "metadata": {},
   "source": [
    "#### Method 1: Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072f710-e618-4c39-afec-020bd02dd7ea",
   "metadata": {},
   "source": [
    "Find replacement words for OOV based on cosine similarity. The steps are:\n",
    "1. Converting the OOV word into a vector (by summing the embeddings of its subwords or by using a neighboring word’s embedding).\n",
    "2. Selecting replacement candidates based on similarity in the vector space.\n",
    "3. Calculating cosine similarity between the OOV word and replacement candidates.\n",
    "4. Choosing a candidate with a high cosine similarity score, ideally close to 1, as the replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fad9f94f-8418-4a00-8c5a-b4d88749c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find replacement words based on cosine similarity\n",
    "def find_replacement_word(oov_word, model, topn=5):\n",
    "    if oov_word in model.wv:\n",
    "        return oov_word  # No replacement needed if word exists in vocab\n",
    "    else:\n",
    "        # Generate vector for OOV by averaging embeddings of subwords (as a simple example)\n",
    "        oov_vector = np.mean([model.wv[word] for word in oov_word if word in model.wv], axis=0)\n",
    "        \n",
    "        # Find similar words to this estimated vector\n",
    "        similar_words = model.wv.similar_by_vector(oov_vector, topn=topn)\n",
    "        \n",
    "        # Return the word with highest similarity\n",
    "        return similar_words[0][0] if similar_words else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef068b80-05de-4521-bfc3-71f41d746092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacement word for OOV: any\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "oov_word = \"algorithm\"\n",
    "replacement = find_replacement_word(oov_word, word2vec_model)\n",
    "print(\"Replacement word for OOV:\", replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b576517-8334-428a-b839-0ed7f85879ef",
   "metadata": {},
   "source": [
    "Advantages: Quick way to handle OOV words by finding a close semantic replacement within the existing vocabulary.   \n",
    "Drawback: Can mistakenly replace an OOV word with a semantically opposite word since cosine similarity alone doesn't fully account for nuanced semantic relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf4bf0-13e4-422c-af00-afb22d7dd7c2",
   "metadata": {},
   "source": [
    "#### Method 2: Subword Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd70db-817e-42c7-aa03-d233ff7466d4",
   "metadata": {},
   "source": [
    "This approach involves breaking down words into smaller units, such as character n-grams, so that even if the entire word isn’t in the vocabulary, its smaller components might be. This can help represent OOV words by capturing their subword structure, which often contains useful semantic information.   \n",
    "\n",
    "With `gensim`, you can implement a similar concept using the **FastText** model, which is capable of learning embeddings for subword units. This model will generate embeddings for any word, even if it wasn’t in the original training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae0026-6597-4d85-86c9-f4568d93a3f2",
   "metadata": {},
   "source": [
    "**Step 1: Train a FastText Model on Your Dataset**   \n",
    "FastText is an extension of Word2vec but includes character n-grams, allowing it to produce embeddings for OOV words by combining the embeddings of their n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eb9640f-d39d-4e2e-a74d-65224ccf25bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size with FastText: 8841\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Set parameters for the FastText model\n",
    "embedding_dim = 100  # Adjust this dimension as needed\n",
    "window_size = 5      # Context window size\n",
    "min_count = 2        # Ignores words with total frequency lower than this\n",
    "\n",
    "# Train the FastText model\n",
    "fasttext_model = FastText(sentences=tokenized_sentences, vector_size=embedding_dim, window=window_size, min_count=min_count, workers=4)\n",
    "\n",
    "# Vocabulary size after training\n",
    "vocab_size = len(fasttext_model.wv)\n",
    "print(\"Vocabulary size with FastText:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adac6fd-0f42-489c-aea3-dd4a31743026",
   "metadata": {},
   "source": [
    "**Step 2: Building the Embedding Matrix for Known Words**   \n",
    "For words in the training data vocabulary, use the learned embeddings. If you encounter an OOV word, FastText will automatically create an embedding based on the subword n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dff5080a-d995-4f2e-9435-e6bcb7165861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape with FastText: (8841, 100)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding matrix for known vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Map words in the FastText vocabulary to the embedding matrix\n",
    "word_to_idx = {word: idx for idx, word in enumerate(fasttext_model.wv.index_to_key)}\n",
    "for word, idx in word_to_idx.items():\n",
    "    embedding_matrix[idx] = fasttext_model.wv[word]\n",
    "\n",
    "print(\"Embedding matrix shape with FastText:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a91114-51fd-4037-98ad-56adea10ae0d",
   "metadata": {},
   "source": [
    "**Step 3: Handle OOV Words in Validation and Test Data**   \n",
    "Using FastText, OOV words should already be covered by the subword embeddings. If a word does not exist in the vocabulary during validation or testing, FastText will still generate an embedding based on the character n-grams it contains, so you can simply use the model as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9af52949-640f-4313-b360-91eb29d484ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'algorithm': [-0.0744204   0.08616591 -0.12863164 -0.08968816  0.21379943  0.13077094\n",
      "  0.01214228  0.20079766  0.10782887 -0.21856885  0.11229397  0.00104024\n",
      " -0.06299437  0.25077713 -0.05319802 -0.08080995  0.03621447 -0.11046183\n",
      " -0.22584887 -0.21995072 -0.30807883  0.04930369 -0.17191687 -0.11598082\n",
      " -0.14564371 -0.21329267 -0.14739008 -0.08986089  0.00861504 -0.00105365\n",
      " -0.09338482  0.01998294  0.23218516 -0.06302945  0.02602169  0.10912537\n",
      "  0.02437454  0.10636683 -0.11935042 -0.19517405  0.16995311 -0.06841488\n",
      "  0.04334502 -0.15479755 -0.14447983 -0.15353383 -0.08981054  0.00958545\n",
      "  0.16006233  0.06367456  0.12022658 -0.09258064 -0.129359   -0.13445319\n",
      " -0.06514408 -0.05390274 -0.11533449 -0.01530871 -0.09887797 -0.02505171\n",
      " -0.16128217 -0.10399985  0.02462547  0.23249193 -0.0537083   0.2611112\n",
      " -0.01397066 -0.00611461  0.03118794  0.09490933 -0.15058059  0.14254603\n",
      "  0.14536972 -0.24163775  0.10825963 -0.06576266  0.1510735  -0.00261503\n",
      " -0.01918464  0.06899466 -0.07378376 -0.1611075  -0.19637242 -0.09127013\n",
      " -0.10352907 -0.28213295  0.16684209  0.08165622 -0.04670113  0.00400829\n",
      " -0.0678545   0.01838551 -0.09975094  0.15953536 -0.14889482  0.16417226\n",
      " -0.01319112 -0.20360455 -0.05158601 -0.0110279 ]\n"
     ]
    }
   ],
   "source": [
    "# Example of getting embeddings for words, including OOV\n",
    "word = \"algorithm\"  # Replace with any OOV word\n",
    "if word in fasttext_model.wv:\n",
    "    embedding = fasttext_model.wv[word]\n",
    "else:\n",
    "    embedding = fasttext_model.wv[word]  # FastText will handle OOV by generating the embedding\n",
    "print(f\"Embedding for '{word}':\", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ef6f9-42ce-493b-bc59-c64cb544129b",
   "metadata": {},
   "source": [
    "- By using FastText, you avoid having to manually handle OOV words because the model can generate embeddings for any word using character n-grams. This enables you to retain useful information from OOV words, as their morphology often reflects their meaning.\n",
    "- This approach is effective without needing transformer-based models, as it leverages the internal structure of words."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
